{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aaa983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a2ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary \n",
    "import onnx \n",
    "import onnxruntime as ort\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098b859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "REPO = \"/home/deepaksr/project/Project_files_2/U-2-Net\"    \n",
    "os.chdir(REPO)\n",
    "sys.path.insert(0, REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b30bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import U2NET, U2NETP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58184d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saliency_RGB_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_RGB_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d833c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saliency_RGB_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_RGB_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== CONFIG ========\n",
    "# Input image folder\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/images/RGB'\n",
    "\n",
    "# Output base folder\n",
    "output_base_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions'\n",
    "\n",
    "# List of models with (name, path)\n",
    "models_info = [\n",
    "    ('u2net', '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_rgb_d1_20250511_201128/best_model.pth'),\n",
    "    ('sal_v1', '/home/deepaksr/project/Project_files_2/training_logs/RGB_v1_d1_fixed_bce_20250514_081726/best_model.pth'),\n",
    "    ('sal_v2', '/home/deepaksr/project/Project_files_2/training_logs/RGB_v2_d1_fixed_bce_20250514_090812/best_model.pth'),\n",
    "]\n",
    "\n",
    "# Image size expected by the model\n",
    "image_size = (320, 320)\n",
    "# =========================\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Prediction saving function\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# Load and run inference for each model\n",
    "for model_name, model_path in models_info:\n",
    "    print(f\"Processing with model: {model_name}\")\n",
    "\n",
    "    # Load model and weights\n",
    "    model = U2NET(3, 1)  # Replace with your specific model class if different\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Create output subfolder for this model\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Process all images\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                d_output = model(input_tensor)\n",
    "                # If model returns multiple outputs (U¬≤-Net does), take the first\n",
    "                if isinstance(d_output, (list, tuple)):\n",
    "                    d_output = d_output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, filename)\n",
    "            save_prediction(d_output, save_path)\n",
    "\n",
    "    print(f\"Saved predictions for {model_name} in {model_output_folder}\")\n",
    "\n",
    "print(\"‚úÖ All model predictions saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f1448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2525057/1559468262.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions/u2net\n",
      "\n",
      "üîÑ Running inference for: sal_v1\n",
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions/sal_v1\n",
      "\n",
      "üîÑ Running inference for: sal_v2\n",
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions/sal_v2\n",
      "\n",
      "üéâ All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_RGB_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_RGB_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class Saliency_RGB_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_RGB_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/images/RGB'\n",
    "output_base_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(3, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_rgb_d1_20250511_201128/best_model.pth',\n",
    "        'input_mode': 'RGB'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_RGB_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/RGB_v1_d1_fixed_bce_20250514_081726/best_model.pth',\n",
    "        'input_mode': 'RGB'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_RGB_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/RGB_v2_d1_fixed_bce_20250514_090812/best_model.pth',\n",
    "        'input_mode': 'RGB'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\nüîÑ Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"‚úÖ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\nüéâ All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0f2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_binary_mask_pred/u2net\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_binary_mask_pred/sal_v1\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_binary_mask_pred/sal_v2\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/saliency_masks'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"rgb_raw_predictions\", \"rgb_binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"rgb_raw_predictions\", \"rgb_comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "\n",
    "        img_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"‚ö†Ô∏è Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('RGB').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(image), np.array(gt), np.array(raw), np.array(binary)]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            # Use color map only for grayscale images\n",
    "            if img.ndim == 2 or img.shape[2] == 1:\n",
    "                ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            else:\n",
    "                ax.imshow(img)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"‚úÖ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"‚úÖ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc2a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder paths\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/images/RGB'\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/saliency_masks'\n",
    "u2netp_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions/u2net'\n",
    "sal_v1_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions/sal_v1'\n",
    "sal_v2_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/rgb_comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U¬≤-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Image resize\n",
    "image_size = (320, 320)\n",
    "\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    input_path = os.path.join(input_folder, base_name + '.jpg')  # adjust extension if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.jpg')        # adjust extension if needed\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"‚ö†Ô∏è Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load input image as RGB, others as grayscale\n",
    "    input_img = np.array(Image.open(input_path).convert('RGB').resize(image_size))\n",
    "    gt_img = np.array(Image.open(gt_path).convert('L').resize(image_size))\n",
    "    u2netp_img = np.array(Image.open(u2netp_path).convert('L').resize(image_size))\n",
    "    sal_v1_img = np.array(Image.open(sal_v1_path).convert('L').resize(image_size))\n",
    "    sal_v2_img = np.array(Image.open(sal_v2_path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [input_img, gt_img, u2netp_img, sal_v1_img, sal_v2_img]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        if img.ndim == 2:  # Grayscale\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        else:  # RGB\n",
    "            ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"‚úÖ Saved model comparison grids to {comparison_grid_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49896be2",
   "metadata": {},
   "source": [
    "data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dcdab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2576842/1392970907.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions/u2net\n",
      "\n",
      "üîÑ Running inference for: sal_v1\n",
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions/sal_v1\n",
      "\n",
      "üîÑ Running inference for: sal_v2\n",
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions/sal_v2\n",
      "\n",
      "üéâ All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_RGB_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_RGB_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class Saliency_RGB_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_RGB_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/images'\n",
    "output_base_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(3, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_rgb_d3_20250511_212906/best_model.pth',\n",
    "        'input_mode': 'RGB'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_RGB_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/RGB_v1_d3_fixed_bce_20250517_153240/best_model.pth',\n",
    "        'input_mode': 'RGB'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_RGB_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/RGB_v2_d3_fixed_bce_20250518_022557/best_model.pth',\n",
    "        'input_mode': 'RGB'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\nüîÑ Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Sort the filenames to ensure consistent ordering\n",
    "    all_filenames = sorted([\n",
    "        fname for fname in os.listdir(input_folder)\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))\n",
    "    ])\n",
    "\n",
    "    # Select one image every 150 steps\n",
    "    selected_filenames = all_filenames[::15]\n",
    "\n",
    "    for fname in selected_filenames:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"‚úÖ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\nüéâ All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e778e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_binary_mask_pred/u2net\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_binary_mask_pred/sal_v1\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_binary_mask_pred/sal_v2\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/saliency_masks'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"rgb_raw_predictions\", \"rgb_binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"rgb_raw_predictions\", \"rgb_comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "\n",
    "        img_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"‚ö†Ô∏è Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('RGB').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(image), np.array(gt), np.array(raw), np.array(binary)]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            # Use color map only for grayscale images\n",
    "            if img.ndim == 2 or img.shape[2] == 1:\n",
    "                ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            else:\n",
    "                ax.imshow(img)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"‚úÖ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"‚úÖ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052c2802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/RGB_only/rgb_comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder paths\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/images'\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/saliency_masks'\n",
    "u2netp_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions/u2net'\n",
    "sal_v1_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions/sal_v1'\n",
    "sal_v2_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/rgb_raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = '/home/deepaksr/project/Saliency_datasets/RGB_only/rgb_comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U¬≤-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Image resize\n",
    "image_size = (320, 320)\n",
    "\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    input_path = os.path.join(input_folder, base_name + '.jpg')  # adjust extension if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.jpg')        # adjust extension if needed\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"‚ö†Ô∏è Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load input image as RGB, others as grayscale\n",
    "    input_img = np.array(Image.open(input_path).convert('RGB').resize(image_size))\n",
    "    gt_img = np.array(Image.open(gt_path).convert('L').resize(image_size))\n",
    "    u2netp_img = np.array(Image.open(u2netp_path).convert('L').resize(image_size))\n",
    "    sal_v1_img = np.array(Image.open(sal_v1_path).convert('L').resize(image_size))\n",
    "    sal_v2_img = np.array(Image.open(sal_v2_path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [input_img, gt_img, u2netp_img, sal_v1_img, sal_v2_img]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        if img.ndim == 2:  # Grayscale\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        else:  # RGB\n",
    "            ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"‚úÖ Saved model comparison grids to {comparison_grid_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826a174",
   "metadata": {},
   "source": [
    "data 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b6eaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the filenames to ensure consistent ordering\n",
    "all_filenames = sorted([\n",
    "    fname for fname in os.listdir(input_folder)\n",
    "    if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))\n",
    "])\n",
    "\n",
    "# Select one image every 150 steps\n",
    "selected_filenames = all_filenames[::150]\n",
    "\n",
    "for fname in selected_filenames:\n",
    "    img_path = os.path.join(input_folder, fname)\n",
    "    image = Image.open(img_path).convert(input_mode)\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        if isinstance(output, (list, tuple)):\n",
    "            output = output[0]\n",
    "\n",
    "    save_path = os.path.join(model_output_folder, fname)\n",
    "    save_prediction(output, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bc6a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/1933183954.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/u2net\n",
      "\n",
      "üîÑ Running inference for: sal_v1\n",
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v1\n",
      "\n",
      "üîÑ Running inference for: sal_v2\n",
      "‚úÖ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v2\n",
      "\n",
      "üéâ All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/images'\n",
    "output_base_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(1, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d3_20250511_212158/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_IR_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d3_fixed_bce_20250517_132705/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_IR_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d3_fixed_bce_20250517_221851/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\nüîÑ Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Sort the filenames to ensure consistent ordering\n",
    "    all_filenames = sorted([\n",
    "        fname for fname in os.listdir(input_folder)\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))\n",
    "    ])\n",
    "\n",
    "    # Select one image every 150 steps\n",
    "    selected_filenames = all_filenames[::150]\n",
    "\n",
    "    for fname in selected_filenames:\n",
    "        \n",
    "    #for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"‚úÖ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\nüéâ All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5b9e712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/binary_mask_pred/u2net\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/binary_mask_pred/sal_v1\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "‚úÖ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/binary_mask_pred/sal_v2\n",
      "‚úÖ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/saliency_masks'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"raw_predictions\", \"binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"raw_predictions\", \"comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "        img_filename = os.path.splitext(filename)[0] + '.png'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.png'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"‚ö†Ô∏è Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('L').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(img) for img in [image, gt, raw, binary]]\n",
    "\n",
    "        # Create subplot grid\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"‚úÖ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"‚úÖ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5268e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed folder paths (update if needed)\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/images'\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/saliency_masks'\n",
    "u2netp_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/u2net'\n",
    "sal_v1_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v1'\n",
    "sal_v2_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U¬≤-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (320, 320)  # Or whatever your working size is\n",
    "\n",
    "# Loop over files in any one prediction folder (they should all match in filenames)\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Paths to all versions\n",
    "    input_path = os.path.join(input_folder, base_name + '.png')  # or .png if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.png')        # or adjust extension\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    # Check all required files exist\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"‚ö†Ô∏è Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and resize all images as grayscale\n",
    "    def load_gray(path):\n",
    "        return np.array(Image.open(path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [load_gray(p) for p in [input_path, gt_path, u2netp_path, sal_v1_path, sal_v2_path]]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"‚úÖ Saved model comparison grids to {comparison_grid_folder}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
