{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aaa983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a2ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary \n",
    "import onnx \n",
    "import onnxruntime as ort\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098b859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "REPO = \"/home/deepaksr/project/Project_files_2/U-2-Net\"    \n",
    "os.chdir(REPO)\n",
    "sys.path.insert(0, REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b30bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import U2NET, U2NETP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4901e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d97737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e35bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/2556978639.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained u2netp from /home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d1_20250511_201145/best_model.pth\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 320, 320]             640\n",
      "       BatchNorm2d-2         [-1, 64, 320, 320]             128\n",
      "              ReLU-3         [-1, 64, 320, 320]               0\n",
      "          REBNCONV-4         [-1, 64, 320, 320]               0\n",
      "            Conv2d-5         [-1, 16, 320, 320]           9,232\n",
      "       BatchNorm2d-6         [-1, 16, 320, 320]              32\n",
      "              ReLU-7         [-1, 16, 320, 320]               0\n",
      "          REBNCONV-8         [-1, 16, 320, 320]               0\n",
      "         MaxPool2d-9         [-1, 16, 160, 160]               0\n",
      "           Conv2d-10         [-1, 16, 160, 160]           2,320\n",
      "      BatchNorm2d-11         [-1, 16, 160, 160]              32\n",
      "             ReLU-12         [-1, 16, 160, 160]               0\n",
      "         REBNCONV-13         [-1, 16, 160, 160]               0\n",
      "        MaxPool2d-14           [-1, 16, 80, 80]               0\n",
      "           Conv2d-15           [-1, 16, 80, 80]           2,320\n",
      "      BatchNorm2d-16           [-1, 16, 80, 80]              32\n",
      "             ReLU-17           [-1, 16, 80, 80]               0\n",
      "         REBNCONV-18           [-1, 16, 80, 80]               0\n",
      "        MaxPool2d-19           [-1, 16, 40, 40]               0\n",
      "           Conv2d-20           [-1, 16, 40, 40]           2,320\n",
      "      BatchNorm2d-21           [-1, 16, 40, 40]              32\n",
      "             ReLU-22           [-1, 16, 40, 40]               0\n",
      "         REBNCONV-23           [-1, 16, 40, 40]               0\n",
      "        MaxPool2d-24           [-1, 16, 20, 20]               0\n",
      "           Conv2d-25           [-1, 16, 20, 20]           2,320\n",
      "      BatchNorm2d-26           [-1, 16, 20, 20]              32\n",
      "             ReLU-27           [-1, 16, 20, 20]               0\n",
      "         REBNCONV-28           [-1, 16, 20, 20]               0\n",
      "        MaxPool2d-29           [-1, 16, 10, 10]               0\n",
      "           Conv2d-30           [-1, 16, 10, 10]           2,320\n",
      "      BatchNorm2d-31           [-1, 16, 10, 10]              32\n",
      "             ReLU-32           [-1, 16, 10, 10]               0\n",
      "         REBNCONV-33           [-1, 16, 10, 10]               0\n",
      "           Conv2d-34           [-1, 16, 10, 10]           2,320\n",
      "      BatchNorm2d-35           [-1, 16, 10, 10]              32\n",
      "             ReLU-36           [-1, 16, 10, 10]               0\n",
      "         REBNCONV-37           [-1, 16, 10, 10]               0\n",
      "           Conv2d-38           [-1, 16, 10, 10]           4,624\n",
      "      BatchNorm2d-39           [-1, 16, 10, 10]              32\n",
      "             ReLU-40           [-1, 16, 10, 10]               0\n",
      "         REBNCONV-41           [-1, 16, 10, 10]               0\n",
      "           Conv2d-42           [-1, 16, 20, 20]           4,624\n",
      "      BatchNorm2d-43           [-1, 16, 20, 20]              32\n",
      "             ReLU-44           [-1, 16, 20, 20]               0\n",
      "         REBNCONV-45           [-1, 16, 20, 20]               0\n",
      "           Conv2d-46           [-1, 16, 40, 40]           4,624\n",
      "      BatchNorm2d-47           [-1, 16, 40, 40]              32\n",
      "             ReLU-48           [-1, 16, 40, 40]               0\n",
      "         REBNCONV-49           [-1, 16, 40, 40]               0\n",
      "           Conv2d-50           [-1, 16, 80, 80]           4,624\n",
      "      BatchNorm2d-51           [-1, 16, 80, 80]              32\n",
      "             ReLU-52           [-1, 16, 80, 80]               0\n",
      "         REBNCONV-53           [-1, 16, 80, 80]               0\n",
      "           Conv2d-54         [-1, 16, 160, 160]           4,624\n",
      "      BatchNorm2d-55         [-1, 16, 160, 160]              32\n",
      "             ReLU-56         [-1, 16, 160, 160]               0\n",
      "         REBNCONV-57         [-1, 16, 160, 160]               0\n",
      "           Conv2d-58         [-1, 64, 320, 320]          18,496\n",
      "      BatchNorm2d-59         [-1, 64, 320, 320]             128\n",
      "             ReLU-60         [-1, 64, 320, 320]               0\n",
      "         REBNCONV-61         [-1, 64, 320, 320]               0\n",
      "             RSU7-62         [-1, 64, 320, 320]               0\n",
      "        MaxPool2d-63         [-1, 64, 160, 160]               0\n",
      "           Conv2d-64         [-1, 64, 160, 160]          36,928\n",
      "      BatchNorm2d-65         [-1, 64, 160, 160]             128\n",
      "             ReLU-66         [-1, 64, 160, 160]               0\n",
      "         REBNCONV-67         [-1, 64, 160, 160]               0\n",
      "           Conv2d-68         [-1, 16, 160, 160]           9,232\n",
      "      BatchNorm2d-69         [-1, 16, 160, 160]              32\n",
      "             ReLU-70         [-1, 16, 160, 160]               0\n",
      "         REBNCONV-71         [-1, 16, 160, 160]               0\n",
      "        MaxPool2d-72           [-1, 16, 80, 80]               0\n",
      "           Conv2d-73           [-1, 16, 80, 80]           2,320\n",
      "      BatchNorm2d-74           [-1, 16, 80, 80]              32\n",
      "             ReLU-75           [-1, 16, 80, 80]               0\n",
      "         REBNCONV-76           [-1, 16, 80, 80]               0\n",
      "        MaxPool2d-77           [-1, 16, 40, 40]               0\n",
      "           Conv2d-78           [-1, 16, 40, 40]           2,320\n",
      "      BatchNorm2d-79           [-1, 16, 40, 40]              32\n",
      "             ReLU-80           [-1, 16, 40, 40]               0\n",
      "         REBNCONV-81           [-1, 16, 40, 40]               0\n",
      "        MaxPool2d-82           [-1, 16, 20, 20]               0\n",
      "           Conv2d-83           [-1, 16, 20, 20]           2,320\n",
      "      BatchNorm2d-84           [-1, 16, 20, 20]              32\n",
      "             ReLU-85           [-1, 16, 20, 20]               0\n",
      "         REBNCONV-86           [-1, 16, 20, 20]               0\n",
      "        MaxPool2d-87           [-1, 16, 10, 10]               0\n",
      "           Conv2d-88           [-1, 16, 10, 10]           2,320\n",
      "      BatchNorm2d-89           [-1, 16, 10, 10]              32\n",
      "             ReLU-90           [-1, 16, 10, 10]               0\n",
      "         REBNCONV-91           [-1, 16, 10, 10]               0\n",
      "           Conv2d-92           [-1, 16, 10, 10]           2,320\n",
      "      BatchNorm2d-93           [-1, 16, 10, 10]              32\n",
      "             ReLU-94           [-1, 16, 10, 10]               0\n",
      "         REBNCONV-95           [-1, 16, 10, 10]               0\n",
      "           Conv2d-96           [-1, 16, 10, 10]           4,624\n",
      "      BatchNorm2d-97           [-1, 16, 10, 10]              32\n",
      "             ReLU-98           [-1, 16, 10, 10]               0\n",
      "         REBNCONV-99           [-1, 16, 10, 10]               0\n",
      "          Conv2d-100           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-101           [-1, 16, 20, 20]              32\n",
      "            ReLU-102           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-103           [-1, 16, 20, 20]               0\n",
      "          Conv2d-104           [-1, 16, 40, 40]           4,624\n",
      "     BatchNorm2d-105           [-1, 16, 40, 40]              32\n",
      "            ReLU-106           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-107           [-1, 16, 40, 40]               0\n",
      "          Conv2d-108           [-1, 16, 80, 80]           4,624\n",
      "     BatchNorm2d-109           [-1, 16, 80, 80]              32\n",
      "            ReLU-110           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-111           [-1, 16, 80, 80]               0\n",
      "          Conv2d-112         [-1, 64, 160, 160]          18,496\n",
      "     BatchNorm2d-113         [-1, 64, 160, 160]             128\n",
      "            ReLU-114         [-1, 64, 160, 160]               0\n",
      "        REBNCONV-115         [-1, 64, 160, 160]               0\n",
      "            RSU6-116         [-1, 64, 160, 160]               0\n",
      "       MaxPool2d-117           [-1, 64, 80, 80]               0\n",
      "          Conv2d-118           [-1, 64, 80, 80]          36,928\n",
      "     BatchNorm2d-119           [-1, 64, 80, 80]             128\n",
      "            ReLU-120           [-1, 64, 80, 80]               0\n",
      "        REBNCONV-121           [-1, 64, 80, 80]               0\n",
      "          Conv2d-122           [-1, 16, 80, 80]           9,232\n",
      "     BatchNorm2d-123           [-1, 16, 80, 80]              32\n",
      "            ReLU-124           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-125           [-1, 16, 80, 80]               0\n",
      "       MaxPool2d-126           [-1, 16, 40, 40]               0\n",
      "          Conv2d-127           [-1, 16, 40, 40]           2,320\n",
      "     BatchNorm2d-128           [-1, 16, 40, 40]              32\n",
      "            ReLU-129           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-130           [-1, 16, 40, 40]               0\n",
      "       MaxPool2d-131           [-1, 16, 20, 20]               0\n",
      "          Conv2d-132           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-133           [-1, 16, 20, 20]              32\n",
      "            ReLU-134           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-135           [-1, 16, 20, 20]               0\n",
      "       MaxPool2d-136           [-1, 16, 10, 10]               0\n",
      "          Conv2d-137           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-138           [-1, 16, 10, 10]              32\n",
      "            ReLU-139           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-140           [-1, 16, 10, 10]               0\n",
      "          Conv2d-141           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-142           [-1, 16, 10, 10]              32\n",
      "            ReLU-143           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-144           [-1, 16, 10, 10]               0\n",
      "          Conv2d-145           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-146           [-1, 16, 10, 10]              32\n",
      "            ReLU-147           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-148           [-1, 16, 10, 10]               0\n",
      "          Conv2d-149           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-150           [-1, 16, 20, 20]              32\n",
      "            ReLU-151           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-152           [-1, 16, 20, 20]               0\n",
      "          Conv2d-153           [-1, 16, 40, 40]           4,624\n",
      "     BatchNorm2d-154           [-1, 16, 40, 40]              32\n",
      "            ReLU-155           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-156           [-1, 16, 40, 40]               0\n",
      "          Conv2d-157           [-1, 64, 80, 80]          18,496\n",
      "     BatchNorm2d-158           [-1, 64, 80, 80]             128\n",
      "            ReLU-159           [-1, 64, 80, 80]               0\n",
      "        REBNCONV-160           [-1, 64, 80, 80]               0\n",
      "            RSU5-161           [-1, 64, 80, 80]               0\n",
      "       MaxPool2d-162           [-1, 64, 40, 40]               0\n",
      "          Conv2d-163           [-1, 64, 40, 40]          36,928\n",
      "     BatchNorm2d-164           [-1, 64, 40, 40]             128\n",
      "            ReLU-165           [-1, 64, 40, 40]               0\n",
      "        REBNCONV-166           [-1, 64, 40, 40]               0\n",
      "          Conv2d-167           [-1, 16, 40, 40]           9,232\n",
      "     BatchNorm2d-168           [-1, 16, 40, 40]              32\n",
      "            ReLU-169           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-170           [-1, 16, 40, 40]               0\n",
      "       MaxPool2d-171           [-1, 16, 20, 20]               0\n",
      "          Conv2d-172           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-173           [-1, 16, 20, 20]              32\n",
      "            ReLU-174           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-175           [-1, 16, 20, 20]               0\n",
      "       MaxPool2d-176           [-1, 16, 10, 10]               0\n",
      "          Conv2d-177           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-178           [-1, 16, 10, 10]              32\n",
      "            ReLU-179           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-180           [-1, 16, 10, 10]               0\n",
      "          Conv2d-181           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-182           [-1, 16, 10, 10]              32\n",
      "            ReLU-183           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-184           [-1, 16, 10, 10]               0\n",
      "          Conv2d-185           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-186           [-1, 16, 10, 10]              32\n",
      "            ReLU-187           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-188           [-1, 16, 10, 10]               0\n",
      "          Conv2d-189           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-190           [-1, 16, 20, 20]              32\n",
      "            ReLU-191           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-192           [-1, 16, 20, 20]               0\n",
      "          Conv2d-193           [-1, 64, 40, 40]          18,496\n",
      "     BatchNorm2d-194           [-1, 64, 40, 40]             128\n",
      "            ReLU-195           [-1, 64, 40, 40]               0\n",
      "        REBNCONV-196           [-1, 64, 40, 40]               0\n",
      "            RSU4-197           [-1, 64, 40, 40]               0\n",
      "       MaxPool2d-198           [-1, 64, 20, 20]               0\n",
      "          Conv2d-199           [-1, 64, 20, 20]          36,928\n",
      "     BatchNorm2d-200           [-1, 64, 20, 20]             128\n",
      "            ReLU-201           [-1, 64, 20, 20]               0\n",
      "        REBNCONV-202           [-1, 64, 20, 20]               0\n",
      "          Conv2d-203           [-1, 16, 20, 20]           9,232\n",
      "     BatchNorm2d-204           [-1, 16, 20, 20]              32\n",
      "            ReLU-205           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-206           [-1, 16, 20, 20]               0\n",
      "          Conv2d-207           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-208           [-1, 16, 20, 20]              32\n",
      "            ReLU-209           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-210           [-1, 16, 20, 20]               0\n",
      "          Conv2d-211           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-212           [-1, 16, 20, 20]              32\n",
      "            ReLU-213           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-214           [-1, 16, 20, 20]               0\n",
      "          Conv2d-215           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-216           [-1, 16, 20, 20]              32\n",
      "            ReLU-217           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-218           [-1, 16, 20, 20]               0\n",
      "          Conv2d-219           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-220           [-1, 16, 20, 20]              32\n",
      "            ReLU-221           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-222           [-1, 16, 20, 20]               0\n",
      "          Conv2d-223           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-224           [-1, 16, 20, 20]              32\n",
      "            ReLU-225           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-226           [-1, 16, 20, 20]               0\n",
      "          Conv2d-227           [-1, 64, 20, 20]          18,496\n",
      "     BatchNorm2d-228           [-1, 64, 20, 20]             128\n",
      "            ReLU-229           [-1, 64, 20, 20]               0\n",
      "        REBNCONV-230           [-1, 64, 20, 20]               0\n",
      "           RSU4F-231           [-1, 64, 20, 20]               0\n",
      "       MaxPool2d-232           [-1, 64, 10, 10]               0\n",
      "          Conv2d-233           [-1, 64, 10, 10]          36,928\n",
      "     BatchNorm2d-234           [-1, 64, 10, 10]             128\n",
      "            ReLU-235           [-1, 64, 10, 10]               0\n",
      "        REBNCONV-236           [-1, 64, 10, 10]               0\n",
      "          Conv2d-237           [-1, 16, 10, 10]           9,232\n",
      "     BatchNorm2d-238           [-1, 16, 10, 10]              32\n",
      "            ReLU-239           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-240           [-1, 16, 10, 10]               0\n",
      "          Conv2d-241           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-242           [-1, 16, 10, 10]              32\n",
      "            ReLU-243           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-244           [-1, 16, 10, 10]               0\n",
      "          Conv2d-245           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-246           [-1, 16, 10, 10]              32\n",
      "            ReLU-247           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-248           [-1, 16, 10, 10]               0\n",
      "          Conv2d-249           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-250           [-1, 16, 10, 10]              32\n",
      "            ReLU-251           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-252           [-1, 16, 10, 10]               0\n",
      "          Conv2d-253           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-254           [-1, 16, 10, 10]              32\n",
      "            ReLU-255           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-256           [-1, 16, 10, 10]               0\n",
      "          Conv2d-257           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-258           [-1, 16, 10, 10]              32\n",
      "            ReLU-259           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-260           [-1, 16, 10, 10]               0\n",
      "          Conv2d-261           [-1, 64, 10, 10]          18,496\n",
      "     BatchNorm2d-262           [-1, 64, 10, 10]             128\n",
      "            ReLU-263           [-1, 64, 10, 10]               0\n",
      "        REBNCONV-264           [-1, 64, 10, 10]               0\n",
      "           RSU4F-265           [-1, 64, 10, 10]               0\n",
      "          Conv2d-266           [-1, 64, 20, 20]          73,792\n",
      "     BatchNorm2d-267           [-1, 64, 20, 20]             128\n",
      "            ReLU-268           [-1, 64, 20, 20]               0\n",
      "        REBNCONV-269           [-1, 64, 20, 20]               0\n",
      "          Conv2d-270           [-1, 16, 20, 20]           9,232\n",
      "     BatchNorm2d-271           [-1, 16, 20, 20]              32\n",
      "            ReLU-272           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-273           [-1, 16, 20, 20]               0\n",
      "          Conv2d-274           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-275           [-1, 16, 20, 20]              32\n",
      "            ReLU-276           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-277           [-1, 16, 20, 20]               0\n",
      "          Conv2d-278           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-279           [-1, 16, 20, 20]              32\n",
      "            ReLU-280           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-281           [-1, 16, 20, 20]               0\n",
      "          Conv2d-282           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-283           [-1, 16, 20, 20]              32\n",
      "            ReLU-284           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-285           [-1, 16, 20, 20]               0\n",
      "          Conv2d-286           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-287           [-1, 16, 20, 20]              32\n",
      "            ReLU-288           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-289           [-1, 16, 20, 20]               0\n",
      "          Conv2d-290           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-291           [-1, 16, 20, 20]              32\n",
      "            ReLU-292           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-293           [-1, 16, 20, 20]               0\n",
      "          Conv2d-294           [-1, 64, 20, 20]          18,496\n",
      "     BatchNorm2d-295           [-1, 64, 20, 20]             128\n",
      "            ReLU-296           [-1, 64, 20, 20]               0\n",
      "        REBNCONV-297           [-1, 64, 20, 20]               0\n",
      "           RSU4F-298           [-1, 64, 20, 20]               0\n",
      "          Conv2d-299           [-1, 64, 40, 40]          73,792\n",
      "     BatchNorm2d-300           [-1, 64, 40, 40]             128\n",
      "            ReLU-301           [-1, 64, 40, 40]               0\n",
      "        REBNCONV-302           [-1, 64, 40, 40]               0\n",
      "          Conv2d-303           [-1, 16, 40, 40]           9,232\n",
      "     BatchNorm2d-304           [-1, 16, 40, 40]              32\n",
      "            ReLU-305           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-306           [-1, 16, 40, 40]               0\n",
      "       MaxPool2d-307           [-1, 16, 20, 20]               0\n",
      "          Conv2d-308           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-309           [-1, 16, 20, 20]              32\n",
      "            ReLU-310           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-311           [-1, 16, 20, 20]               0\n",
      "       MaxPool2d-312           [-1, 16, 10, 10]               0\n",
      "          Conv2d-313           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-314           [-1, 16, 10, 10]              32\n",
      "            ReLU-315           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-316           [-1, 16, 10, 10]               0\n",
      "          Conv2d-317           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-318           [-1, 16, 10, 10]              32\n",
      "            ReLU-319           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-320           [-1, 16, 10, 10]               0\n",
      "          Conv2d-321           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-322           [-1, 16, 10, 10]              32\n",
      "            ReLU-323           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-324           [-1, 16, 10, 10]               0\n",
      "          Conv2d-325           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-326           [-1, 16, 20, 20]              32\n",
      "            ReLU-327           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-328           [-1, 16, 20, 20]               0\n",
      "          Conv2d-329           [-1, 64, 40, 40]          18,496\n",
      "     BatchNorm2d-330           [-1, 64, 40, 40]             128\n",
      "            ReLU-331           [-1, 64, 40, 40]               0\n",
      "        REBNCONV-332           [-1, 64, 40, 40]               0\n",
      "            RSU4-333           [-1, 64, 40, 40]               0\n",
      "          Conv2d-334           [-1, 64, 80, 80]          73,792\n",
      "     BatchNorm2d-335           [-1, 64, 80, 80]             128\n",
      "            ReLU-336           [-1, 64, 80, 80]               0\n",
      "        REBNCONV-337           [-1, 64, 80, 80]               0\n",
      "          Conv2d-338           [-1, 16, 80, 80]           9,232\n",
      "     BatchNorm2d-339           [-1, 16, 80, 80]              32\n",
      "            ReLU-340           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-341           [-1, 16, 80, 80]               0\n",
      "       MaxPool2d-342           [-1, 16, 40, 40]               0\n",
      "          Conv2d-343           [-1, 16, 40, 40]           2,320\n",
      "     BatchNorm2d-344           [-1, 16, 40, 40]              32\n",
      "            ReLU-345           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-346           [-1, 16, 40, 40]               0\n",
      "       MaxPool2d-347           [-1, 16, 20, 20]               0\n",
      "          Conv2d-348           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-349           [-1, 16, 20, 20]              32\n",
      "            ReLU-350           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-351           [-1, 16, 20, 20]               0\n",
      "       MaxPool2d-352           [-1, 16, 10, 10]               0\n",
      "          Conv2d-353           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-354           [-1, 16, 10, 10]              32\n",
      "            ReLU-355           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-356           [-1, 16, 10, 10]               0\n",
      "          Conv2d-357           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-358           [-1, 16, 10, 10]              32\n",
      "            ReLU-359           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-360           [-1, 16, 10, 10]               0\n",
      "          Conv2d-361           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-362           [-1, 16, 10, 10]              32\n",
      "            ReLU-363           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-364           [-1, 16, 10, 10]               0\n",
      "          Conv2d-365           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-366           [-1, 16, 20, 20]              32\n",
      "            ReLU-367           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-368           [-1, 16, 20, 20]               0\n",
      "          Conv2d-369           [-1, 16, 40, 40]           4,624\n",
      "     BatchNorm2d-370           [-1, 16, 40, 40]              32\n",
      "            ReLU-371           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-372           [-1, 16, 40, 40]               0\n",
      "          Conv2d-373           [-1, 64, 80, 80]          18,496\n",
      "     BatchNorm2d-374           [-1, 64, 80, 80]             128\n",
      "            ReLU-375           [-1, 64, 80, 80]               0\n",
      "        REBNCONV-376           [-1, 64, 80, 80]               0\n",
      "            RSU5-377           [-1, 64, 80, 80]               0\n",
      "          Conv2d-378         [-1, 64, 160, 160]          73,792\n",
      "     BatchNorm2d-379         [-1, 64, 160, 160]             128\n",
      "            ReLU-380         [-1, 64, 160, 160]               0\n",
      "        REBNCONV-381         [-1, 64, 160, 160]               0\n",
      "          Conv2d-382         [-1, 16, 160, 160]           9,232\n",
      "     BatchNorm2d-383         [-1, 16, 160, 160]              32\n",
      "            ReLU-384         [-1, 16, 160, 160]               0\n",
      "        REBNCONV-385         [-1, 16, 160, 160]               0\n",
      "       MaxPool2d-386           [-1, 16, 80, 80]               0\n",
      "          Conv2d-387           [-1, 16, 80, 80]           2,320\n",
      "     BatchNorm2d-388           [-1, 16, 80, 80]              32\n",
      "            ReLU-389           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-390           [-1, 16, 80, 80]               0\n",
      "       MaxPool2d-391           [-1, 16, 40, 40]               0\n",
      "          Conv2d-392           [-1, 16, 40, 40]           2,320\n",
      "     BatchNorm2d-393           [-1, 16, 40, 40]              32\n",
      "            ReLU-394           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-395           [-1, 16, 40, 40]               0\n",
      "       MaxPool2d-396           [-1, 16, 20, 20]               0\n",
      "          Conv2d-397           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-398           [-1, 16, 20, 20]              32\n",
      "            ReLU-399           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-400           [-1, 16, 20, 20]               0\n",
      "       MaxPool2d-401           [-1, 16, 10, 10]               0\n",
      "          Conv2d-402           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-403           [-1, 16, 10, 10]              32\n",
      "            ReLU-404           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-405           [-1, 16, 10, 10]               0\n",
      "          Conv2d-406           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-407           [-1, 16, 10, 10]              32\n",
      "            ReLU-408           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-409           [-1, 16, 10, 10]               0\n",
      "          Conv2d-410           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-411           [-1, 16, 10, 10]              32\n",
      "            ReLU-412           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-413           [-1, 16, 10, 10]               0\n",
      "          Conv2d-414           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-415           [-1, 16, 20, 20]              32\n",
      "            ReLU-416           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-417           [-1, 16, 20, 20]               0\n",
      "          Conv2d-418           [-1, 16, 40, 40]           4,624\n",
      "     BatchNorm2d-419           [-1, 16, 40, 40]              32\n",
      "            ReLU-420           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-421           [-1, 16, 40, 40]               0\n",
      "          Conv2d-422           [-1, 16, 80, 80]           4,624\n",
      "     BatchNorm2d-423           [-1, 16, 80, 80]              32\n",
      "            ReLU-424           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-425           [-1, 16, 80, 80]               0\n",
      "          Conv2d-426         [-1, 64, 160, 160]          18,496\n",
      "     BatchNorm2d-427         [-1, 64, 160, 160]             128\n",
      "            ReLU-428         [-1, 64, 160, 160]               0\n",
      "        REBNCONV-429         [-1, 64, 160, 160]               0\n",
      "            RSU6-430         [-1, 64, 160, 160]               0\n",
      "          Conv2d-431         [-1, 64, 320, 320]          73,792\n",
      "     BatchNorm2d-432         [-1, 64, 320, 320]             128\n",
      "            ReLU-433         [-1, 64, 320, 320]               0\n",
      "        REBNCONV-434         [-1, 64, 320, 320]               0\n",
      "          Conv2d-435         [-1, 16, 320, 320]           9,232\n",
      "     BatchNorm2d-436         [-1, 16, 320, 320]              32\n",
      "            ReLU-437         [-1, 16, 320, 320]               0\n",
      "        REBNCONV-438         [-1, 16, 320, 320]               0\n",
      "       MaxPool2d-439         [-1, 16, 160, 160]               0\n",
      "          Conv2d-440         [-1, 16, 160, 160]           2,320\n",
      "     BatchNorm2d-441         [-1, 16, 160, 160]              32\n",
      "            ReLU-442         [-1, 16, 160, 160]               0\n",
      "        REBNCONV-443         [-1, 16, 160, 160]               0\n",
      "       MaxPool2d-444           [-1, 16, 80, 80]               0\n",
      "          Conv2d-445           [-1, 16, 80, 80]           2,320\n",
      "     BatchNorm2d-446           [-1, 16, 80, 80]              32\n",
      "            ReLU-447           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-448           [-1, 16, 80, 80]               0\n",
      "       MaxPool2d-449           [-1, 16, 40, 40]               0\n",
      "          Conv2d-450           [-1, 16, 40, 40]           2,320\n",
      "     BatchNorm2d-451           [-1, 16, 40, 40]              32\n",
      "            ReLU-452           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-453           [-1, 16, 40, 40]               0\n",
      "       MaxPool2d-454           [-1, 16, 20, 20]               0\n",
      "          Conv2d-455           [-1, 16, 20, 20]           2,320\n",
      "     BatchNorm2d-456           [-1, 16, 20, 20]              32\n",
      "            ReLU-457           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-458           [-1, 16, 20, 20]               0\n",
      "       MaxPool2d-459           [-1, 16, 10, 10]               0\n",
      "          Conv2d-460           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-461           [-1, 16, 10, 10]              32\n",
      "            ReLU-462           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-463           [-1, 16, 10, 10]               0\n",
      "          Conv2d-464           [-1, 16, 10, 10]           2,320\n",
      "     BatchNorm2d-465           [-1, 16, 10, 10]              32\n",
      "            ReLU-466           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-467           [-1, 16, 10, 10]               0\n",
      "          Conv2d-468           [-1, 16, 10, 10]           4,624\n",
      "     BatchNorm2d-469           [-1, 16, 10, 10]              32\n",
      "            ReLU-470           [-1, 16, 10, 10]               0\n",
      "        REBNCONV-471           [-1, 16, 10, 10]               0\n",
      "          Conv2d-472           [-1, 16, 20, 20]           4,624\n",
      "     BatchNorm2d-473           [-1, 16, 20, 20]              32\n",
      "            ReLU-474           [-1, 16, 20, 20]               0\n",
      "        REBNCONV-475           [-1, 16, 20, 20]               0\n",
      "          Conv2d-476           [-1, 16, 40, 40]           4,624\n",
      "     BatchNorm2d-477           [-1, 16, 40, 40]              32\n",
      "            ReLU-478           [-1, 16, 40, 40]               0\n",
      "        REBNCONV-479           [-1, 16, 40, 40]               0\n",
      "          Conv2d-480           [-1, 16, 80, 80]           4,624\n",
      "     BatchNorm2d-481           [-1, 16, 80, 80]              32\n",
      "            ReLU-482           [-1, 16, 80, 80]               0\n",
      "        REBNCONV-483           [-1, 16, 80, 80]               0\n",
      "          Conv2d-484         [-1, 16, 160, 160]           4,624\n",
      "     BatchNorm2d-485         [-1, 16, 160, 160]              32\n",
      "            ReLU-486         [-1, 16, 160, 160]               0\n",
      "        REBNCONV-487         [-1, 16, 160, 160]               0\n",
      "          Conv2d-488         [-1, 64, 320, 320]          18,496\n",
      "     BatchNorm2d-489         [-1, 64, 320, 320]             128\n",
      "            ReLU-490         [-1, 64, 320, 320]               0\n",
      "        REBNCONV-491         [-1, 64, 320, 320]               0\n",
      "            RSU7-492         [-1, 64, 320, 320]               0\n",
      "          Conv2d-493          [-1, 1, 320, 320]             577\n",
      "          Conv2d-494          [-1, 1, 160, 160]             577\n",
      "          Conv2d-495            [-1, 1, 80, 80]             577\n",
      "          Conv2d-496            [-1, 1, 40, 40]             577\n",
      "          Conv2d-497            [-1, 1, 20, 20]             577\n",
      "          Conv2d-498            [-1, 1, 10, 10]             577\n",
      "          Conv2d-499          [-1, 1, 320, 320]               7\n",
      "================================================================\n",
      "Total params: 1,130,029\n",
      "Trainable params: 1,130,029\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 1452.90\n",
      "Params size (MB): 4.31\n",
      "Estimated Total Size (MB): 1457.60\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"u2netp\"   # or \"u2net\"\n",
    "ModelClass = U2NETP if model_name==\"u2netp\" else U2NET\n",
    "\n",
    "model = ModelClass(1,1).to(device)\n",
    "pth = \"/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d1_20250511_201145/best_model.pth\"\n",
    "model.load_state_dict(torch.load(pth, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Loaded pretrained {model_name} from {pth}\")\n",
    "\n",
    "# summary \n",
    "summary(model, input_size=(1,320,320), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57462f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained u2netp from /home/deepaksr/project/Project_files_2/training_logs/IR_v1_d1_fixed_bce_20250514_091441/best_model.pth\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 320, 320]             640\n",
      "              ReLU-2         [-1, 64, 320, 320]               0\n",
      "         MaxPool2d-3         [-1, 64, 160, 160]               0\n",
      "            Conv2d-4        [-1, 128, 160, 160]          73,856\n",
      "              ReLU-5        [-1, 128, 160, 160]               0\n",
      "         MaxPool2d-6          [-1, 128, 80, 80]               0\n",
      "            Conv2d-7          [-1, 256, 80, 80]         295,168\n",
      "              ReLU-8          [-1, 256, 80, 80]               0\n",
      "         MaxPool2d-9          [-1, 256, 40, 40]               0\n",
      "  ConvTranspose2d-10          [-1, 128, 80, 80]         131,200\n",
      "             ReLU-11          [-1, 128, 80, 80]               0\n",
      "  ConvTranspose2d-12         [-1, 64, 160, 160]          32,832\n",
      "             ReLU-13         [-1, 64, 160, 160]               0\n",
      "  ConvTranspose2d-14          [-1, 1, 320, 320]             257\n",
      "          Sigmoid-15          [-1, 1, 320, 320]               0\n",
      "================================================================\n",
      "Total params: 533,953\n",
      "Trainable params: 533,953\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 235.94\n",
      "Params size (MB): 2.04\n",
      "Estimated Total Size (MB): 238.36\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/2766731883.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Saliency_IR_v1().to(device)\n",
    "pth = \"/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d1_fixed_bce_20250514_091441/best_model.pth\"\n",
    "model.load_state_dict(torch.load(pth, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Loaded pretrained {model_name} from {pth}\")\n",
    "\n",
    "# summary \n",
    "summary(model, input_size=(1,320,320), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8775ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained u2netp from /home/deepaksr/project/Project_files_2/training_logs/IR_v2_d1_fixed_bce_20250514_105129/best_model.pth\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 320, 320]              80\n",
      "              ReLU-2          [-1, 8, 320, 320]               0\n",
      "         MaxPool2d-3          [-1, 8, 160, 160]               0\n",
      "            Conv2d-4         [-1, 16, 160, 160]           1,168\n",
      "              ReLU-5         [-1, 16, 160, 160]               0\n",
      "         MaxPool2d-6           [-1, 16, 80, 80]               0\n",
      "            Conv2d-7           [-1, 32, 80, 80]           4,640\n",
      "              ReLU-8           [-1, 32, 80, 80]               0\n",
      "         MaxPool2d-9           [-1, 32, 40, 40]               0\n",
      "           Conv2d-10           [-1, 64, 40, 40]          18,496\n",
      "             ReLU-11           [-1, 64, 40, 40]               0\n",
      "        MaxPool2d-12           [-1, 64, 20, 20]               0\n",
      "  ConvTranspose2d-13           [-1, 32, 40, 40]           8,224\n",
      "             ReLU-14           [-1, 32, 40, 40]               0\n",
      "  ConvTranspose2d-15           [-1, 16, 80, 80]           2,064\n",
      "             ReLU-16           [-1, 16, 80, 80]               0\n",
      "  ConvTranspose2d-17          [-1, 8, 160, 160]             520\n",
      "             ReLU-18          [-1, 8, 160, 160]               0\n",
      "  ConvTranspose2d-19          [-1, 1, 320, 320]              33\n",
      "          Sigmoid-20          [-1, 1, 320, 320]               0\n",
      "================================================================\n",
      "Total params: 35,225\n",
      "Trainable params: 35,225\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 33.40\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 33.92\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/2196028285.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pth, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Saliency_IR_v2().to(device)\n",
    "pth = \"/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d1_fixed_bce_20250514_105129/best_model.pth\"\n",
    "model.load_state_dict(torch.load(pth, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Loaded pretrained {model_name} from {pth}\")\n",
    "\n",
    "# summary \n",
    "summary(model, input_size=(1,320,320), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== CONFIG ========\n",
    "# Input image folder\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/images/norm_ir_8bit'\n",
    "\n",
    "# Output base folder\n",
    "output_base_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions'\n",
    "\n",
    "# List of models with (name, path)\n",
    "models_info = [\n",
    "    ('u2net', '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d1_20250511_201145/best_model.pth'),\n",
    "    ('sal_v1', '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d1_fixed_bce_20250514_091441/best_model.pth'),\n",
    "    ('sal_v2', '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d1_fixed_bce_20250514_105129/best_model.pth'),\n",
    "]\n",
    "\n",
    "# Image size expected by the model\n",
    "image_size = (320, 320)\n",
    "# =========================\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Prediction saving function\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# Load and run inference for each model\n",
    "for model_name, model_path in models_info:\n",
    "    print(f\"Processing with model: {model_name}\")\n",
    "\n",
    "    # Load model and weights\n",
    "    model = U2NET(3, 1)  # Replace with your specific model class if different\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Create output subfolder for this model\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Process all images\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                d_output = model(input_tensor)\n",
    "                # If model returns multiple outputs (U²-Net does), take the first\n",
    "                if isinstance(d_output, (list, tuple)):\n",
    "                    d_output = d_output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, filename)\n",
    "            save_prediction(d_output, save_path)\n",
    "\n",
    "    print(f\"Saved predictions for {model_name} in {model_output_folder}\")\n",
    "\n",
    "print(\"✅ All model predictions saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f1448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/2605625289.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions/u2net\n",
      "\n",
      "🔄 Running inference for: sal_v1\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions/sal_v1\n",
      "\n",
      "🔄 Running inference for: sal_v2\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions/sal_v2\n",
      "\n",
      "🎉 All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/images/norm_ir_8bit'\n",
    "output_base_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(1, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d1_20250511_201145/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_IR_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d1_fixed_bce_20250514_091441/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_IR_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d1_fixed_bce_20250514_105129/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\n🔄 Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"✅ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\n🎉 All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa0f2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/binary_mask_pred/u2net\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/binary_mask_pred/sal_v1\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/binary_mask_pred/sal_v2\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/saliency_masks'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"raw_predictions\", \"binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"raw_predictions\", \"comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"⚠️ Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('L').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(img) for img in [image, gt, raw, binary]]\n",
    "\n",
    "        # Create subplot grid\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"✅ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1eae504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/co-reg/simulated/comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed folder paths (update if needed)\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/images/norm_ir_8bit'\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/saliency_masks'\n",
    "u2netp_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions/u2net'\n",
    "sal_v1_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions/sal_v1'\n",
    "sal_v2_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/simulated/comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U²-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (320, 320)  # Or whatever your working size is\n",
    "\n",
    "# Loop over files in any one prediction folder (they should all match in filenames)\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Paths to all versions\n",
    "    input_path = os.path.join(input_folder, base_name + '.png')  # or .png if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.jpg')        # or adjust extension\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    # Check all required files exist\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"⚠️ Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and resize all images as grayscale\n",
    "    def load_gray(path):\n",
    "        return np.array(Image.open(path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [load_gray(p) for p in [input_path, gt_path, u2netp_path, sal_v1_path, sal_v2_path]]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✅ Saved model comparison grids to {comparison_grid_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49896be2",
   "metadata": {},
   "source": [
    "data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e69eebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/379659670.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions/u2net\n",
      "\n",
      "🔄 Running inference for: sal_v1\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions/sal_v1\n",
      "\n",
      "🔄 Running inference for: sal_v2\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions/sal_v2\n",
      "\n",
      "🎉 All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/images/IR_norm8bit'\n",
    "output_base_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(1, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d2_20250511_210042/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_IR_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d2_fixed_bce_20250515_131724/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_IR_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d2_fixed_bce_20250515_152205/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\n🔄 Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"✅ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\n🎉 All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "732cab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/binary_mask_pred/u2net\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/binary_mask_pred/sal_v1\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/binary_mask_pred/sal_v2\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/Saliency_map/IR'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"raw_predictions\", \"binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"raw_predictions\", \"comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "        img_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"⚠️ Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('L').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(img) for img in [image, gt, raw, binary]]\n",
    "\n",
    "        # Create subplot grid\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"✅ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ddce0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed folder paths (update if needed)\n",
    "input_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/images/IR_norm8bit'\n",
    "gt_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/Saliency_map/IR'\n",
    "u2netp_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions/u2net'\n",
    "sal_v1_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions/sal_v1'\n",
    "sal_v2_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = '/home/deepaksr/project/Saliency_datasets/co-reg/paper_dataset/comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U²-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (320, 320)  # Or whatever your working size is\n",
    "\n",
    "# Loop over files in any one prediction folder (they should all match in filenames)\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Paths to all versions\n",
    "    input_path = os.path.join(input_folder, base_name + '.jpg')  # or .png if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.jpg')        # or adjust extension\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    # Check all required files exist\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"⚠️ Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and resize all images as grayscale\n",
    "    def load_gray(path):\n",
    "        return np.array(Image.open(path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [load_gray(p) for p in [input_path, gt_path, u2netp_path, sal_v1_path, sal_v2_path]]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✅ Saved model comparison grids to {comparison_grid_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ea772",
   "metadata": {},
   "source": [
    "data 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc460a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/3871419775.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions/u2net\n",
      "\n",
      "🔄 Running inference for: sal_v1\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions/sal_v1\n",
      "\n",
      "🔄 Running inference for: sal_v2\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions/sal_v2\n",
      "\n",
      "🎉 All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/images'\n",
    "output_base_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(1, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d4_20250512_010041/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_IR_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d4_fixed_bce_20250515_164139/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_IR_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d4_fixed_bce_20250515_171405/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\n🔄 Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"✅ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\n🎉 All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "354685a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/binary_mask_pred/u2net\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/binary_mask_pred/sal_v1\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/binary_mask_pred/sal_v2\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/saliency_masks'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"raw_predictions\", \"binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"raw_predictions\", \"comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "        img_filename = os.path.splitext(filename)[0] + '.png'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.png'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"⚠️ Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('L').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(img) for img in [image, gt, raw, binary]]\n",
    "\n",
    "        # Create subplot grid\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"✅ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "604acab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed folder paths (update if needed)\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/images'\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/saliency_masks'\n",
    "u2netp_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions/u2net'\n",
    "sal_v1_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions/sal_v1'\n",
    "sal_v2_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Bridge data/comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U²-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (320, 320)  # Or whatever your working size is\n",
    "\n",
    "# Loop over files in any one prediction folder (they should all match in filenames)\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Paths to all versions\n",
    "    input_path = os.path.join(input_folder, base_name + '.png')  # or .png if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.png')        # or adjust extension\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    # Check all required files exist\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"⚠️ Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and resize all images as grayscale\n",
    "    def load_gray(path):\n",
    "        return np.array(Image.open(path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [load_gray(p) for p in [input_path, gt_path, u2netp_path, sal_v1_path, sal_v2_path]]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✅ Saved model comparison grids to {comparison_grid_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057491f4",
   "metadata": {},
   "source": [
    "data 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0ddf7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/1931188259.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions/u2net\n",
      "\n",
      "🔄 Running inference for: sal_v1\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions/sal_v1\n",
      "\n",
      "🔄 Running inference for: sal_v2\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions/sal_v2\n",
      "\n",
      "🎉 All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/images/train'\n",
    "output_base_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(1, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d5_20250512_011730/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_IR_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d5_fixed_bce_20250515_173850/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_IR_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d5_fixed_bce_20250515_181709/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\n🔄 Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"✅ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\n🎉 All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0961d234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/binary_mask_pred/u2net\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/binary_mask_pred/sal_v1\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/binary_mask_pred/sal_v2\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/saliency_masks/train'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"raw_predictions\", \"binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"raw_predictions\", \"comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "        img_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"⚠️ Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('L').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(img) for img in [image, gt, raw, binary]]\n",
    "\n",
    "        # Create subplot grid\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"✅ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19b17c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed folder paths (update if needed)\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/images/train'\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/saliency_masks/train'\n",
    "u2netp_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions/u2net'\n",
    "sal_v1_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions/sal_v1'\n",
    "sal_v2_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/hit-uav/comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U²-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (320, 320)  # Or whatever your working size is\n",
    "\n",
    "# Loop over files in any one prediction folder (they should all match in filenames)\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Paths to all versions\n",
    "    input_path = os.path.join(input_folder, base_name + '.jpg')  # or .png if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.jpg')        # or adjust extension\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    # Check all required files exist\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"⚠️ Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and resize all images as grayscale\n",
    "    def load_gray(path):\n",
    "        return np.array(Image.open(path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [load_gray(p) for p in [input_path, gt_path, u2netp_path, sal_v1_path, sal_v2_path]]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✅ Saved model comparison grids to {comparison_grid_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826a174",
   "metadata": {},
   "source": [
    "data 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b6eaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the filenames to ensure consistent ordering\n",
    "all_filenames = sorted([\n",
    "    fname for fname in os.listdir(input_folder)\n",
    "    if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))\n",
    "])\n",
    "\n",
    "# Select one image every 150 steps\n",
    "selected_filenames = all_filenames[::150]\n",
    "\n",
    "for fname in selected_filenames:\n",
    "    img_path = os.path.join(input_folder, fname)\n",
    "    image = Image.open(img_path).convert(input_mode)\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        if isinstance(output, (list, tuple)):\n",
    "            output = output[0]\n",
    "\n",
    "    save_path = os.path.join(model_output_folder, fname)\n",
    "    save_prediction(output, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bc6a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Running inference for: u2net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381364/1933183954.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
      "/home/deepaksr/project/Project_files_2/U-2-Net/model/u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/u2net\n",
      "\n",
      "🔄 Running inference for: sal_v1\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v1\n",
      "\n",
      "🔄 Running inference for: sal_v2\n",
      "✅ Saved predictions in /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v2\n",
      "\n",
      "🎉 All model predictions completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import U2NET, U2NETP\n",
    "\n",
    "# === Define Custom IR Models ===\n",
    "class Saliency_IR_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Saliency_IR_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Saliency_IR_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=2, stride=2), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# === CONFIG ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/images'\n",
    "output_base_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions'\n",
    "image_size = (320, 320)\n",
    "\n",
    "# === Define model loading info ===\n",
    "models_info = {\n",
    "    'u2net': {\n",
    "        'model_class': lambda: U2NETP(1, 1),\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/U-2-Net/training_logs/u2netp_finetune_ir_d3_20250511_212158/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v1': {\n",
    "        'model_class': Saliency_IR_v1,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v1_d3_fixed_bce_20250517_132705/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    },\n",
    "    'sal_v2': {\n",
    "        'model_class': Saliency_IR_v2,\n",
    "        'weight_path': '/home/deepaksr/project/Project_files_2/training_logs/IR_v2_d3_fixed_bce_20250517_221851/best_model.pth',\n",
    "        'input_mode': 'L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Transform Function ===\n",
    "def get_transform(input_mode):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# === Save prediction ===\n",
    "def save_prediction(output_tensor, save_path):\n",
    "    output = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output = (output * 255).astype(np.uint8)\n",
    "    Image.fromarray(output).save(save_path)\n",
    "\n",
    "# === Inference Loop ===\n",
    "for model_name, info in models_info.items():\n",
    "    print(f\"\\n🔄 Running inference for: {model_name}\")\n",
    "    \n",
    "    model = info['model_class']().to(device)\n",
    "    model.load_state_dict(torch.load(info['weight_path'], map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Set correct image mode (L for grayscale)\n",
    "    input_mode = info['input_mode']\n",
    "    transform = get_transform(input_mode)\n",
    "\n",
    "    # Prepare output directory\n",
    "    model_output_folder = os.path.join(output_base_folder, model_name)\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Sort the filenames to ensure consistent ordering\n",
    "    all_filenames = sorted([\n",
    "        fname for fname in os.listdir(input_folder)\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))\n",
    "    ])\n",
    "\n",
    "    # Select one image every 150 steps\n",
    "    selected_filenames = all_filenames[::150]\n",
    "\n",
    "    for fname in selected_filenames:\n",
    "        \n",
    "    #for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            img_path = os.path.join(input_folder, fname)\n",
    "            image = Image.open(img_path).convert(input_mode)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    output = output[0]\n",
    "\n",
    "            save_path = os.path.join(model_output_folder, fname)\n",
    "            save_prediction(output, save_path)\n",
    "\n",
    "    print(f\"✅ Saved predictions in {model_output_folder}\")\n",
    "\n",
    "print(\"\\n🎉 All model predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5b9e712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing outputs for model: u2net\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/binary_mask_pred/u2net\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison/u2net\n",
      "Post-processing outputs for model: sal_v1\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/binary_mask_pred/sal_v1\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison/sal_v1\n",
      "Post-processing outputs for model: sal_v2\n",
      "✅ Saved binary predictions to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/binary_mask_pred/sal_v2\n",
      "✅ Saved visual comparisons to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison/sal_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Folder paths\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/saliency_masks'  # Update with actual GT path\n",
    "binary_base_folder = output_base_folder.replace(\"raw_predictions\", \"binary_mask_pred\")\n",
    "comparison_base_folder = output_base_folder.replace(\"raw_predictions\", \"comparison\")\n",
    "\n",
    "titles = ['Input Image', 'Ground Truth', 'Raw Prediction', 'Binary Prediction']\n",
    "\n",
    "for model_name in models_info:\n",
    "    print(f\"Post-processing outputs for model: {model_name}\")\n",
    "\n",
    "    raw_folder = os.path.join(output_base_folder, model_name)\n",
    "    binary_folder = os.path.join(binary_base_folder, model_name)\n",
    "    comparison_folder = os.path.join(comparison_base_folder, model_name)\n",
    "\n",
    "    os.makedirs(binary_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(raw_folder):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            continue\n",
    "\n",
    "        raw_path = os.path.join(raw_folder, filename)\n",
    "        raw_pred = Image.open(raw_path).convert('L')\n",
    "        raw_np = np.array(raw_pred) / 255.0\n",
    "        binary_np = (raw_np > 0.5).astype(np.uint8) * 255\n",
    "        binary_img = Image.fromarray(binary_np)\n",
    "\n",
    "        binary_path = os.path.join(binary_folder, filename)\n",
    "        binary_img.save(binary_path)\n",
    "        img_filename = os.path.splitext(filename)[0] + '.png'\n",
    "        image_path = os.path.join(input_folder, img_filename)\n",
    "        gt_filename = os.path.splitext(filename)[0] + '.png'\n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"⚠️ Ground truth mask not found for {gt_filename}, skipping visualization.\")\n",
    "            continue\n",
    "\n",
    "        # Load and resize all images (grayscale)\n",
    "        image = Image.open(image_path).convert('L').resize(image_size)\n",
    "        gt = Image.open(gt_path).convert('L').resize(image_size)\n",
    "        raw = raw_pred.resize(image_size)\n",
    "        binary = binary_img.resize(image_size)\n",
    "\n",
    "        images = [np.array(img) for img in [image, gt, raw, binary]]\n",
    "\n",
    "        # Create subplot grid\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(comparison_folder, filename)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Saved binary predictions to {binary_folder}\")\n",
    "    print(f\"✅ Saved visual comparisons to {comparison_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5268e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model comparison grids to /home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison_across_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed folder paths (update if needed)\n",
    "input_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/images'\n",
    "gt_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/saliency_masks'\n",
    "u2netp_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/u2net'\n",
    "sal_v1_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v1'\n",
    "sal_v2_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/raw_predictions/sal_v2'\n",
    "\n",
    "# Output folder\n",
    "comparison_grid_folder = r'/home/deepaksr/project/Saliency_datasets/IR_only/Actual scenario/comparison_across_models'\n",
    "os.makedirs(comparison_grid_folder, exist_ok=True)\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Input Image', 'Ground Truth', 'U²-NetP', 'Saliency_v1', 'Saliency_v2']\n",
    "\n",
    "# Set the desired image size\n",
    "image_size = (320, 320)  # Or whatever your working size is\n",
    "\n",
    "# Loop over files in any one prediction folder (they should all match in filenames)\n",
    "for filename in os.listdir(u2netp_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Paths to all versions\n",
    "    input_path = os.path.join(input_folder, base_name + '.png')  # or .png if needed\n",
    "    gt_path = os.path.join(gt_folder, base_name + '.png')        # or adjust extension\n",
    "    u2netp_path = os.path.join(u2netp_folder, filename)\n",
    "    sal_v1_path = os.path.join(sal_v1_folder, filename)\n",
    "    sal_v2_path = os.path.join(sal_v2_folder, filename)\n",
    "\n",
    "    # Check all required files exist\n",
    "    if not (os.path.exists(input_path) and os.path.exists(gt_path) and \n",
    "            os.path.exists(u2netp_path) and os.path.exists(sal_v1_path) and os.path.exists(sal_v2_path)):\n",
    "        print(f\"⚠️ Missing files for {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and resize all images as grayscale\n",
    "    def load_gray(path):\n",
    "        return np.array(Image.open(path).convert('L').resize(image_size))\n",
    "\n",
    "    images = [load_gray(p) for p in [input_path, gt_path, u2netp_path, sal_v1_path, sal_v2_path]]\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_grid_folder, filename)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✅ Saved model comparison grids to {comparison_grid_folder}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
