{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4100023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to model mapping\n",
    "FOLDER_MODEL_MAP = {\n",
    "    \"IR Simulated\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/simulated/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/simulated_dataset/u2netp_finetuned_d1.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d1/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR Actual_Scenarios\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/actual_ir/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/actual_ir/u2netp_finetuned_d3.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR Vehicle_Scenarios\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/vehicle_ir/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/vehicle_ir/u2netp_finetuned_d4.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d4/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR HIT-UAV_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/hit-uav/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/hit-uav/u2netp_finetuned_d5.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d5/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR MilPaper_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/paper/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/paper_dataset/u2netp_finetuned_d2.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d2/train_n/weights/best.pt\"\n",
    "    },\n",
    "        \"RGB Simulated\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/simulated/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/simulated_dataset/u2netp_finetuned_rgb_d1.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d1/train_n/weights/best.pt\"\n",
    "    },\n",
    "        \"RGB Military_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/RGB_d3/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/military_dataset/u2netp_finetuned_rgb_d3.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "        \"RGB MilPaper_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/paper/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/paper_dataset/u2netp_finetuned_rgb_d2.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d2/train_n/weights/best.pt\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fc170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://60ddc696c4dffb8e95.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://60ddc696c4dffb8e95.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 23.9ms\n",
      "Speed: 2.8ms preprocess, 23.9ms inference, 72.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 3.1ms\n",
      "Speed: 0.7ms preprocess, 3.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 3.5ms\n",
      "Speed: 0.6ms preprocess, 3.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 3.3ms\n",
      "Speed: 0.7ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3.2ms\n",
      "Speed: 0.6ms preprocess, 3.2ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 Persons, 1 JCB, 2 Vehicles, 25.8ms\n",
      "Speed: 1.3ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 Persons, 1 JCB, 2 Vehicles, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Persons, 1 JCB, 2 Vehicles, 3.4ms\n",
      "Speed: 1.0ms preprocess, 3.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 Persons, 1 JCB, 2 Vehicles, 3.4ms\n",
      "Speed: 1.0ms preprocess, 3.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Persons, 1 JCB, 2 Vehicles, 4.7ms\n",
      "Speed: 0.9ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 Vehicles, 1 Car, 2 Two Wheelers, 27.0ms\n",
      "Speed: 0.6ms preprocess, 27.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 Vehicles, 1 Car, 1 Two Wheeler, 3.3ms\n",
      "Speed: 0.7ms preprocess, 3.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 Vehicles, 2 Cars, 1 Two Wheeler, 3.7ms\n",
      "Speed: 1.0ms preprocess, 3.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Vehicle, 4 Cars, 1 Two Wheeler, 3.3ms\n",
      "Speed: 0.7ms preprocess, 3.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Vehicle, 4 Cars, 1 Two Wheeler, 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 12 Cars, 25.8ms\n",
      "Speed: 1.1ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 9 Cars, 3.5ms\n",
      "Speed: 0.8ms preprocess, 3.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 12 Cars, 3.8ms\n",
      "Speed: 0.7ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 4 Cars, 3.4ms\n",
      "Speed: 0.7ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 5 Persons, 6.9ms\n",
      "Speed: 0.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x640 1 Truck, 4 Cars, 25.2ms\n",
      "Speed: 1.1ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x640 2 Persons, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 512x640 1 Truck, 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 1 Truck, 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 Truck, 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 26.4ms\n",
      "Speed: 0.6ms preprocess, 26.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 3.3ms\n",
      "Speed: 0.7ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 3.0ms\n",
      "Speed: 0.6ms preprocess, 3.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 6.8ms\n",
      "Speed: 0.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 Cars, 3.8ms\n",
      "Speed: 0.7ms preprocess, 3.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Folder to model mapping\n",
    "FOLDER_MODEL_MAP = {\n",
    "    \"IR Simulated\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/simulated/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/simulated_dataset/u2netp_finetuned_d1.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d1/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR Actual_Scenarios\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/actual_ir/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/actual_ir/u2netp_finetuned_d3.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR Vehicle_Scenarios\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/vehicle_ir/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/vehicle_ir/u2netp_finetuned_d4.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d4/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR HIT-UAV_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/hit-uav/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/hit-uav/u2netp_finetuned_d5.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d5/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR MilPaper_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/paper/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/paper_dataset/u2netp_finetuned_d2.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d2/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"RGB Simulated\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/simulated/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/simulated_dataset/u2netp_finetuned_rgb_d1.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d1/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"RGB Military_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/RGB_d3/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/military_dataset/u2netp_finetuned_rgb_d3.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"RGB MilPaper_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/paper/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/paper_dataset/u2netp_finetuned_rgb_d2.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d2/train_n/weights/best.pt\"\n",
    "    },\n",
    "}\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Helper: Load images from a folder, with appropriate channel conversion\n",
    "def load_images_from_folder(folder_key, num_images):\n",
    "    info = FOLDER_MODEL_MAP[folder_key]\n",
    "    files = sorted([f for f in os.listdir(info['path']) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])[:num_images]\n",
    "    images = []\n",
    "    for fname in files:\n",
    "        img_path = os.path.join(info['path'], fname)\n",
    "        if folder_key.startswith('IR'):\n",
    "            # Load grayscale for saliency, but keep a 3-channel copy for detection\n",
    "            gray = Image.open(img_path).convert('L')\n",
    "            rgb = gray.convert('RGB')\n",
    "            images.append((gray, rgb, fname))\n",
    "        else:\n",
    "            # RGB\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            images.append((img, img, fname))\n",
    "    return images\n",
    "\n",
    "# Load saliency model (TorchScript)\n",
    "def load_saliency_model(path):\n",
    "    sal_model = torch.jit.load(path, map_location=device).eval()\n",
    "    return sal_model\n",
    "\n",
    "# Load detection model (Ultralytics YOLO)\n",
    "def load_detection_model(path):\n",
    "    det_model = YOLO(path)\n",
    "    return det_model\n",
    "\n",
    "# Run saliency inference\n",
    "def run_saliency(model, image):\n",
    "    # image: PIL L mode\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    x = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    sal = torch.sigmoid(out[0]).squeeze().cpu().numpy()\n",
    "    sal = (sal * 255).astype(np.uint8)\n",
    "    return Image.fromarray(sal)\n",
    "\n",
    "# Run detection inference\n",
    "def run_detection(model, image):\n",
    "    # image: PIL RGB mode\n",
    "    np_img = np.array(image)\n",
    "    results = model(np_img)[0]\n",
    "    img_out = np_img.copy()\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        score = float(box.conf[0])\n",
    "        cls = int(box.cls[0])\n",
    "        if score > 0.5:\n",
    "            cv2.rectangle(img_out, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img_out, f\"{cls}:{score:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 1)\n",
    "    return Image.fromarray(img_out)\n",
    "\n",
    "# Inference pipeline\n",
    "def inference(folder_key, num_images):\n",
    "    # Load images and models\n",
    "    data = load_images_from_folder(folder_key, num_images)\n",
    "    sal_model = load_saliency_model(FOLDER_MODEL_MAP[folder_key]['saliency_model'])\n",
    "    det_model = load_detection_model(FOLDER_MODEL_MAP[folder_key]['detection_model'])\n",
    "\n",
    "    outputs = []\n",
    "    for sal_img, det_img, fname in data:\n",
    "        sal_map = run_saliency(sal_model, sal_img)\n",
    "        det_res = run_detection(det_model, det_img)\n",
    "        outputs.extend([\n",
    "            (det_img, f\"Input: {fname}\"),\n",
    "            (sal_map, \"Saliency Map\"),\n",
    "            (det_res, \"Detection Result\")\n",
    "        ])\n",
    "    return outputs\n",
    "\n",
    "# Build Gradio interface\n",
    "def create_demo():\n",
    "    folder_list = list(FOLDER_MODEL_MAP.keys())\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Multi-Modal Surveillance Demo - Image Mode\")\n",
    "        with gr.Row():\n",
    "            folder_dd = gr.Dropdown(folder_list, label=\"Select Folder\")\n",
    "            num_sl = gr.Slider(1, 20, value=5, step=1, label=\"Number of Images\")\n",
    "        run_btn = gr.Button(\"Run Inference\")\n",
    "        gallery = gr.Gallery(label=\"Results\", columns=3, object_fit=\"contain\")\n",
    "        run_btn.click(inference, inputs=[folder_dd, num_sl], outputs=gallery)\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = create_demo()\n",
    "    app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840eb4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* Running on public URL: https://bd9be3dcf4eadefd7b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bd9be3dcf4eadefd7b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 21.8ms\n",
      "Speed: 1.0ms preprocess, 21.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 2.7ms\n",
      "Speed: 0.6ms preprocess, 2.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 2.5ms\n",
      "Speed: 0.6ms preprocess, 2.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 2.6ms\n",
      "Speed: 0.6ms preprocess, 2.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Person, 2.5ms\n",
      "Speed: 0.6ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 Persons, 1 JCB, 2 Vehicles, 22.2ms\n",
      "Speed: 0.7ms preprocess, 22.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 Persons, 23.1ms\n",
      "Speed: 0.7ms preprocess, 23.1ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 5 Persons, 1 JCB, 2 Vehicles, 3.0ms\n",
      "Speed: 0.7ms preprocess, 3.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 6.5ms\n",
      "Speed: 0.8ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Person, 2.6ms\n",
      "Speed: 0.5ms preprocess, 2.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 22.4ms\n",
      "Speed: 0.7ms preprocess, 22.4ms inference, 0.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 Persons, 2.4ms\n",
      "Speed: 0.6ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Person, 2.4ms\n",
      "Speed: 0.5ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Person, 2.5ms\n",
      "Speed: 0.5ms preprocess, 2.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 2.4ms\n",
      "Speed: 0.6ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Folder to model mapping\n",
    "FOLDER_MODEL_MAP = {\n",
    "    \"IR Simulated\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/simulated/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/simulated_dataset/u2netp_finetuned_d1.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d1/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR Actual_Scenarios\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/actual_ir/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/actual_ir/u2netp_finetuned_d3.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR Vehicle_Scenarios\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/vehicle_ir/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/vehicle_ir/u2netp_finetuned_d4.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d4/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR HIT-UAV_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/hit-uav/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/hit-uav/u2netp_finetuned_d5.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d5/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"IR MilPaper_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/paper/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/paper_dataset/u2netp_finetuned_d2.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d2/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"RGB Simulated\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/simulated/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/simulated_dataset/u2netp_finetuned_rgb_d1.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d1/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"RGB Military_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/RGB_d3/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/military_dataset/u2netp_finetuned_rgb_d3.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"RGB MilPaper_Dataset\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/paper/images/test\",\n",
    "        \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/paper_dataset/u2netp_finetuned_rgb_d2.pt\",\n",
    "        \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d2/train_n/weights/best.pt\"\n",
    "    },\n",
    "}\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Helper: Load random images from a folder, with appropriate channel conversion\n",
    "def load_images_from_folder(folder_key, num_images):\n",
    "    info = FOLDER_MODEL_MAP[folder_key]\n",
    "    all_files = [f for f in os.listdir(info['path']) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    selected = random.sample(all_files, min(num_images, len(all_files)))\n",
    "    images = []\n",
    "    for fname in selected:\n",
    "        img_path = os.path.join(info['path'], fname)\n",
    "        if folder_key.startswith('IR'):\n",
    "            gray = Image.open(img_path).convert('L')\n",
    "            rgb = gray.convert('RGB')\n",
    "            images.append((gray, rgb, fname))\n",
    "        else:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            images.append((img, img, fname))\n",
    "    return images\n",
    "\n",
    "# Load saliency model (TorchScript)\n",
    "def load_saliency_model(path):\n",
    "    return torch.jit.load(path, map_location=device).eval()\n",
    "\n",
    "# Load detection model (Ultralytics YOLO)\n",
    "def load_detection_model(path):\n",
    "    model = YOLO(path)\n",
    "    return model\n",
    "\n",
    "# Run saliency inference\n",
    "def run_saliency(model, image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    x = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    sal = torch.sigmoid(out[0]).squeeze().cpu().numpy()\n",
    "    sal = (sal * 255).astype(np.uint8)\n",
    "    return Image.fromarray(sal)\n",
    "\n",
    "# Run detection inference\n",
    "def run_detection(model, image):\n",
    "    np_img = np.array(image)\n",
    "    results = model(np_img)[0]\n",
    "    img_out = np_img.copy()\n",
    "    names = model.names\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        score = float(box.conf[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "        if score > 0.5:\n",
    "            cls_name = names.get(cls_id, str(cls_id))\n",
    "            cv2.rectangle(img_out, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img_out, f\"{cls_name}:{score:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 1)\n",
    "    return Image.fromarray(img_out)\n",
    "\n",
    "# Inference pipeline\n",
    "def inference(folder_key, num_images):\n",
    "    data = load_images_from_folder(folder_key, num_images)\n",
    "    sal_model = load_saliency_model(FOLDER_MODEL_MAP[folder_key]['saliency_model'])\n",
    "    det_model = load_detection_model(FOLDER_MODEL_MAP[folder_key]['detection_model'])\n",
    "\n",
    "    outputs = []\n",
    "    for sal_img, det_img, fname in data:\n",
    "        sal_map = run_saliency(sal_model, sal_img)\n",
    "        det_res = run_detection(det_model, det_img)\n",
    "        outputs.extend([\n",
    "            (det_img, f\"Input: {fname}\"),\n",
    "            (sal_map, \"Saliency Map\"),\n",
    "            (det_res, \"Detection Result\")\n",
    "        ])\n",
    "    return outputs\n",
    "\n",
    "# Build Gradio interface\n",
    "def create_demo():\n",
    "    folder_list = list(FOLDER_MODEL_MAP.keys())\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Multi-Modal Surveillance Demo - Image Mode\")\n",
    "        with gr.Row():\n",
    "            folder_dd = gr.Dropdown(folder_list, label=\"Select Folder\")\n",
    "            num_sl = gr.Slider(1, 20, value=5, step=1, label=\"Number of Images\")\n",
    "        run_btn = gr.Button(\"Run Inference\")\n",
    "        gallery = gr.Gallery(label=\"Results\", columns=3, object_fit=\"contain\")\n",
    "        run_btn.click(inference, inputs=[folder_dd, num_sl], outputs=gallery)\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = create_demo()\n",
    "    app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389df761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* Running on public URL: https://63a3e6fcb4ac34a2bb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://63a3e6fcb4ac34a2bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Car, 22.2ms\n",
      "Speed: 0.6ms preprocess, 22.2ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Person, 3.5ms\n",
      "Speed: 0.9ms preprocess, 3.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Person, 2.5ms\n",
      "Speed: 0.6ms preprocess, 2.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 2.6ms\n",
      "Speed: 0.6ms preprocess, 2.6ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 Car, 22.0ms\n",
      "Speed: 1.3ms preprocess, 22.0ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Car, 2.5ms\n",
      "Speed: 0.6ms preprocess, 2.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "code/__torch__/torch/nn/functional/___torch_mangle_33.py:8: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Folder to model mapping\n",
    "FOLDER_MODEL_MAP = {\n",
    "    \"IR Simulated\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/simulated/images/test\",\n",
    "                       \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/simulated_dataset/u2netp_finetuned_d1.pt\",\n",
    "                       \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d1/train_n/weights/best.pt\"},\n",
    "    \"IR Actual_Scenarios\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/actual_ir/images/test\",\n",
    "                             \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/actual_ir/u2netp_finetuned_d3.pt\",\n",
    "                             \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d3/train_n/weights/best.pt\"},\n",
    "    \"IR Vehicle_Scenarios\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/vehicle_ir/images/test\",\n",
    "                              \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/vehicle_ir/u2netp_finetuned_d4.pt\",\n",
    "                              \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d4/train_n/weights/best.pt\"},\n",
    "    \"IR HIT-UAV_Dataset\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/hit-uav/images/test\",\n",
    "                            \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/hit-uav/u2netp_finetuned_d5.pt\",\n",
    "                            \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d5/train_n/weights/best.pt\"},\n",
    "    \"IR MilPaper_Dataset\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/IR/paper/images/test\",\n",
    "                             \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/IR/paper_dataset/u2netp_finetuned_d2.pt\",\n",
    "                             \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d2/train_n/weights/best.pt\"},\n",
    "    \"RGB Simulated\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/simulated/images/test\",\n",
    "                       \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/simulated_dataset/u2netp_finetuned_rgb_d1.pt\",\n",
    "                       \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d1/train_n/weights/best.pt\"},\n",
    "    \"RGB Military_Dataset\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/RGB_d3/images/test\",\n",
    "                              \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/military_dataset/u2netp_finetuned_rgb_d3.pt\",\n",
    "                              \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d3/train_n/weights/best.pt\"},\n",
    "    \"RGB MilPaper_Dataset\": {\"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/RGB/paper/images/test\",\n",
    "                              \"saliency_model\": \"/home/samy/shivam/dsr_project/saliency_models_pt/RGB/paper_dataset/u2netp_finetuned_rgb_d2.pt\",\n",
    "                              \"detection_model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_rgb_d2/train_n/weights/best.pt\"},\n",
    "}\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Load and sample images\n",
    "def load_images_from_folder(folder_key, num_images):\n",
    "    info = FOLDER_MODEL_MAP[folder_key]\n",
    "    all_files = [f for f in os.listdir(info['path']) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    selected = random.sample(all_files, min(num_images, len(all_files)))\n",
    "    data = []\n",
    "    for fname in selected:\n",
    "        img_path = os.path.join(info['path'], fname)\n",
    "        if folder_key.startswith('IR'):\n",
    "            gray = Image.open(img_path).convert('L')\n",
    "            rgb = gray.convert('RGB')\n",
    "            data.append((gray, rgb, fname))\n",
    "        else:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            data.append((img, img, fname))\n",
    "    return data\n",
    "\n",
    "# Load models\n",
    "load_saliency_model = lambda p: torch.jit.load(p, map_location=device).eval()\n",
    "load_detection_model = lambda p: YOLO(p)\n",
    "\n",
    "# Preprocessing for saliency\n",
    "transform_sal = transforms.Compose([transforms.Resize((320,320)), transforms.ToTensor()])\n",
    "\n",
    "# Run saliency and binarize directly on probability map\n",
    "def run_saliency(model, image, bin_thresh):\n",
    "    x = transform_sal(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outs = model(x)\n",
    "    # assume outs[0] is probability map in [0,1]\n",
    "    sal = outs[0].squeeze().cpu().numpy()\n",
    "    bin_mask = (sal >= bin_thresh).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bin_mask), bin_mask\n",
    "\n",
    "# Run detection\n",
    "def run_detection(model, image):\n",
    "    np_img = np.array(image)\n",
    "    results = model(np_img)[0]\n",
    "    img_out = np_img.copy()\n",
    "    names = model.names\n",
    "    for box in results.boxes:\n",
    "        x1,y1,x2,y2 = map(int, box.xyxy[0])\n",
    "        score = float(box.conf[0]); cls = int(box.cls[0])\n",
    "        if score>0.5:\n",
    "            cls_name = names.get(cls, str(cls))\n",
    "            cv2.rectangle(img_out, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "            cv2.putText(img_out, f\"{cls_name}:{score:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 1)\n",
    "    return Image.fromarray(img_out)\n",
    "\n",
    "# Create a text image for \"No object presence\"\n",
    "def text_image(text, size=(320,320)):\n",
    "    img = Image.new('RGB', size, color=(255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    try:\n",
    "        font = ImageFont.load_default()\n",
    "        w, h = font.getsize(text)\n",
    "    except Exception:\n",
    "        bbox = draw.textbbox((0,0), text)\n",
    "        w, h = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "    draw.text(((size[0]-w)/2, (size[1]-h)/2), text, fill=(0,0,0), font=font)\n",
    "    return img\n",
    "\n",
    "# Inference pipeline\n",
    "def inference(folder_key, num_images, bin_thresh, min_frac):\n",
    "    data = load_images_from_folder(folder_key, num_images)\n",
    "    sal_model = load_saliency_model(FOLDER_MODEL_MAP[folder_key]['saliency_model'])\n",
    "    det_model = load_detection_model(FOLDER_MODEL_MAP[folder_key]['detection_model'])\n",
    "    outputs = []\n",
    "    for sal_img, det_img, fname in data:\n",
    "        bin_mask_img, bin_mask = run_saliency(sal_model, sal_img, bin_thresh)\n",
    "        frac = bin_mask.sum() / (bin_mask.size * 255)\n",
    "        outputs.append((det_img, f\"Input: {fname}\"))\n",
    "        outputs.append((bin_mask_img, f\"Binarized Saliency (Thresh={bin_thresh})\"))\n",
    "        if frac >= min_frac:\n",
    "            det_res = run_detection(det_model, det_img)\n",
    "            outputs.append((det_res, f\"Detection (Sens frac={frac:.3f})\"))\n",
    "        else:\n",
    "            no_img = text_image(\"No object presence detected\")\n",
    "            outputs.append((no_img, f\"Insignificant (Sens frac={frac:.3f})\"))\n",
    "    return outputs\n",
    "\n",
    "# Build Gradio interface\n",
    "def create_demo():\n",
    "    folder_list = list(FOLDER_MODEL_MAP.keys())\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Surveillance Demo - Image Mode\")\n",
    "        with gr.Row():\n",
    "            folder_dd = gr.Dropdown(folder_list, label=\"Select Folder\")\n",
    "            num_sl = gr.Slider(1,20,value=5,step=1,label=\"Number of Images\")\n",
    "        with gr.Row():\n",
    "            bin_slider = gr.Slider(0.0,1.0,value=0.5,step=0.01,label=\"Saliency Threshold\")\n",
    "            frac_slider = gr.Number(value=0.001,label=\"Min Salient Fraction\")\n",
    "        run_btn = gr.Button(\"Run Inference\")\n",
    "        gallery = gr.Gallery(label=\"Results\", columns=3, object_fit=\"contain\")\n",
    "        run_btn.click(inference, inputs=[folder_dd,num_sl,bin_slider,frac_slider], outputs=gallery)\n",
    "    return demo\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo = create_demo()\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c024d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5cfaab52097f3a1f66.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Mapping from display name to video folder and model\n",
    "VIDEO_MODEL_MAP = {\n",
    "    \"Actual_IR\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/video_demo/Actual_ir\",\n",
    "        \"model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d3/train_n/weights/best.pt\"\n",
    "    },\n",
    "    \"Vehicle_IR\": {\n",
    "        \"path\": \"/home/samy/shivam/dsr_project/salguided_od_datasets/video_demo/Vehicle_ir\",\n",
    "        \"model\": \"/home/samy/shivam/dsr_project/Project_files/runs/yolov11_ir_d4/train_n/weights/best.pt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# List video files in a folder\n",
    "def list_videos(folder):\n",
    "    return [f for f in os.listdir(folder) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
    "\n",
    "# Tracking inference with log capture and output copying\n",
    "def run_tracking(folder_key, video_name, tracker_type):\n",
    "    if not folder_key or not video_name:\n",
    "        return None, \"Error: Please select both a video folder and a video file.\"\n",
    "    info = VIDEO_MODEL_MAP[folder_key]\n",
    "    video_path = os.path.join(info['path'], video_name)\n",
    "    model = YOLO(info['model'])\n",
    "\n",
    "    # Capture logs\n",
    "    buf = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buf\n",
    "\n",
    "    # Determine tracker config\n",
    "    tracker_cfg = tracker_type if tracker_type.endswith('.yaml') else f\"{tracker_type}.yaml\"\n",
    "\n",
    "    # Run tracking\n",
    "    results = model.track(source=video_path, tracker=tracker_cfg, persist=True, save=True)\n",
    "\n",
    "    # Restore stdout\n",
    "    sys.stdout = old_stdout\n",
    "    log_output = buf.getvalue()\n",
    "\n",
    "    # Find the output file in the save directory\n",
    "    save_dir = results[0].save_dir\n",
    "    base_name = os.path.splitext(video_name)[0]\n",
    "    for ext in ['.avi', '.mp4']:\n",
    "        candidate = os.path.join(save_dir, base_name + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            tracked_path = candidate\n",
    "            break\n",
    "    else:\n",
    "        return None, f\"Tracked output not found for: {video_name}\\n{log_output}\"\n",
    "\n",
    "    # Copy to cwd for Gradio\n",
    "    dst_path = os.path.join(os.getcwd(), os.path.basename(tracked_path))\n",
    "    shutil.copyfile(tracked_path, dst_path)\n",
    "    return dst_path, log_output\n",
    "\n",
    "# Build Gradio interface for video demo\n",
    "def create_video_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Surveillance Demo - Video Tracking Mode\")\n",
    "        with gr.Row():\n",
    "            folder_dd = gr.Dropdown(list(VIDEO_MODEL_MAP.keys()), label=\"Select Video Folder\")\n",
    "            video_dd = gr.Dropdown(choices=[], label=\"Select Video File\", allow_custom_value=False)\n",
    "        tracker_radio = gr.Radio(choices=[\"bytetrack\", \"botsort\"], value=\"bytetrack\", label=\"Tracker Type\")\n",
    "        run_btn = gr.Button(\"Run Tracking\")\n",
    "        output_video = gr.Video(label=\"Tracked Output Video\")\n",
    "        log_area = gr.Textbox(label=\"Tracking Logs\", lines=10)\n",
    "\n",
    "        def update_videos(folder_key):\n",
    "            if folder_key in VIDEO_MODEL_MAP:\n",
    "                vids = list_videos(VIDEO_MODEL_MAP[folder_key]['path'])\n",
    "                return gr.update(choices=vids, value=vids[0] if vids else None)\n",
    "            return gr.update(choices=[], value=None)\n",
    "\n",
    "        folder_dd.change(fn=update_videos, inputs=folder_dd, outputs=video_dd)\n",
    "        run_btn.click(fn=run_tracking, inputs=[folder_dd, video_dd, tracker_radio], outputs=[output_video, log_area])\n",
    "    return demo\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo = create_video_demo()\n",
    "    demo.launch(share=True, allowed_paths=[os.getcwd()] + [VIDEO_MODEL_MAP[k]['path'] for k in VIDEO_MODEL_MAP])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shivam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
